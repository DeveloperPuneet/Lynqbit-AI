# main.py
import torch # Import PyTorch ğŸ¤–
import torch.nn as nn # Neural network module ğŸ§ 
import torch.optim as optim # For optimization âš™ï¸
from torch.nn.utils.rnn import pad_sequence # Pad sequences ğŸ“
import random # For randomness ğŸ²
import numpy as np # For numerical ops ğŸ§®
import os # For OS interaction ğŸ’»

# ------------------ Flags ------------------
TRAIN_MODE = False   # set to False if you only want to chat with the saved model

# ------------------ Special tokens ------------------
SOS_TOKEN = "<SOS>" # Start token ğŸ
EOS_TOKEN = "<EOS>" # End token ğŸ
PAD_TOKEN = "<PAD>" # Padding token ğŸ§±

# ------------------ Model parameters ------------------
d_model = 256 # Embedding dimension ğŸ“
num_heads = 8 # Attention heads count ğŸ§ 
num_layers = 4 # Transformer layers count ğŸ§±
max_len = 60 # Maximum sequence length ğŸ“
batch_size = 16 # Batch size ğŸ“¦
temperature = 0.95 # Sampling temperature ğŸŒ¡ï¸
top_p = 0.92 # Nucleus sampling parameter â˜¢ï¸
repetition_penalty = 1.2 # Repetition penalty factor âš–ï¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Use GPU if available ğŸš€

# ------------------ Enhanced Transformer Model ------------------
class ImprovedTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads=8, num_layers=4, max_len=60, dropout=0.1):
        super().__init__()
        self.tok_embed = nn.Embedding(vocab_size, d_model) # Token embeddings ğŸ“
        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model)) # Positional embeddings ğŸ“
        self.dropout = nn.Dropout(dropout) # Dropout layer ğŸ’§
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=num_heads,
            dim_feedforward=d_model * 4,
            batch_first=True,
            dropout=dropout
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers) # Encoder layer ğŸ§±
        
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model, 
            nhead=num_heads,
            dim_feedforward=d_model * 4,
            batch_first=True,
            dropout=dropout
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers) # Decoder layer ğŸ§±
        
        self.out = nn.Linear(d_model, vocab_size) # Output layer ğŸ“¤
        
    def forward(self, src, tgt,
                src_key_padding_mask=None,
                tgt_key_padding_mask=None,
                tgt_mask=None,
                memory_key_padding_mask=None):
        # src: (B, S)  tgt: (B, T)
        src_emb = self.dropout(self.tok_embed(src) + self.pos_embed[:, :src.size(1), :]) # Embed source âœï¸
        tgt_emb = self.dropout(self.tok_embed(tgt) + self.pos_embed[:, :tgt.size(1), :]) # Embed target âœï¸
        
        # pass padding masks to encoder/decoder properly
        memory = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask) # Encode source ğŸ§±
        output = self.decoder(
            tgt_emb,
            memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=src_key_padding_mask
        ) # Decode target ğŸ§±
        return self.out(output) # Output logits ğŸ“¤

# ------------------ Helpers ------------------
def text_to_tensor(words, word2idx):
    idxs = [word2idx.get(w, word2idx[PAD_TOKEN]) for w in words] # Get indices ğŸ”¢
    return torch.tensor(idxs, dtype=torch.long, device=device) # Convert to tensor âœï¸

def tensor_to_text(tensor, idx2word):
    words = [] # Initialize words list ğŸ“
    for i in tensor: # Iterate tensor elements ğŸ”„
        word = idx2word[i.item()] # Get word ğŸ’¬
        if word not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]: # Filter special tokens ğŸš«
            words.append(word) # Add word to list âœ…
    return " ".join(words) # Join words with space ğŸ—£ï¸

def create_mask(src, tgt, word2idx):
    """
    Returns: src_padding_mask (B,S) bool, tgt_padding_mask (B,T) bool, tgt_mask (T,T) float causal mask
    """
    tgt_seq_len = tgt.shape[1] # Target sequence length ğŸ“
    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)  # (T, T) generate mask ğŸ­
    
    # Convert boolean masks to float masks with -inf for masked positions
    src_padding_mask = (src == word2idx[PAD_TOKEN]).to(device)   # (B, S) Create source mask ğŸ­
    tgt_padding_mask = (tgt == word2idx[PAD_TOKEN]).to(device)   # (B, T) Create target mask ğŸ­
    
    # Convert boolean masks to float masks with -inf for masked positions
    src_padding_mask_float = torch.zeros_like(src, dtype=torch.float32, device=device) # Create src mask float ğŸ­
    src_padding_mask_float = src_padding_mask_float.masked_fill(src_padding_mask, float('-inf')) # Fill mask ğŸ­
    
    tgt_padding_mask_float = torch.zeros_like(tgt, dtype=torch.float32, device=device) # Create tgt mask float ğŸ­
    tgt_padding_mask_float = tgt_padding_mask_float.masked_fill(tgt_padding_mask, float('-inf')) # Fill mask ğŸ­
    
    return src_padding_mask_float, tgt_padding_mask_float, tgt_mask # Return masks ğŸ­

# ------------------ Safe nucleus-sampling util ------------------
def top_p_filter(probs, top_p):
    """
    probs: (1, V) probabilities (after softmax)
    returns filtered probs with entries outside nucleus set to 0, renormalized if possible
    """
    # sort descending
    sorted_probs, sorted_indices = torch.sort(probs, descending=True) # Sort probs â¬†ï¸
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1) # Cumulative probs â•
    mask = cumulative_probs > top_p # Create mask ğŸ­
    # keep at least the top token
    mask[..., 0] = False # Always keep top one âœ…
    # map mask back to original indices
    indices_to_remove = torch.zeros_like(mask, dtype=torch.bool).scatter(1, sorted_indices, mask) # Get indices ğŸ”¢
    filtered = probs.masked_fill(indices_to_remove, 0.0) # Apply mask ğŸ­
    if filtered.sum() == 0:
        # fallback: return original probs (no filtering)
        return probs # Return original probs ğŸ”„
    return filtered / filtered.sum() # Normalize filtered probs âš–ï¸

# ------------------ Response generator (used in training & inference) ------------------
def generate_coherent_response(input_text, model, word2idx, idx2word, max_length=30,
                               temperature_local=temperature, top_p_local=top_p,
                               repetition_penalty_local=repetition_penalty):
    model.eval() # Set to eval mode ğŸš¦
    src = text_to_tensor(input_text.split() + [EOS_TOKEN], word2idx).unsqueeze(0)  # (1, S) Prepare source ğŸ“
    tgt = torch.tensor([[word2idx[SOS_TOKEN]]], device=device)  # (1,1) Prepare target ğŸ“
    generated_words, last_tokens = [], [] # Initialize lists ğŸ“

    with torch.no_grad(): # Disable gradient calculation ğŸš«
        for _ in range(max_length): # Generate tokens loop ğŸ”„
            src_padding_mask, tgt_padding_mask, tgt_mask = create_mask(src, tgt, word2idx) # Create masks ğŸ­
            output = model(src, tgt, src_key_padding_mask=src_padding_mask,
                           tgt_key_padding_mask=tgt_padding_mask, tgt_mask=tgt_mask) # Get model output ğŸ“¤
            next_token_logits = output[:, -1, :]  # (1, V) Get next logits ğŸ”¢
            # apply temperature
            next_token_logits = next_token_logits / max(1e-8, temperature_local) # Apply temperature ğŸŒ¡ï¸

            # apply repetition penalty on logits (safer than dividing probabilities)
            for token_id in set(tgt[0].tolist() + last_tokens[-5:]): # Iterate tokens ğŸ”„
                if token_id != word2idx[SOS_TOKEN]: # Filter SOS token ğŸš«
                    next_token_logits[0, token_id] /= (repetition_penalty_local * 1.5) # Apply penalty âš–ï¸

            probs = torch.softmax(next_token_logits, dim=-1)  # (1, V) Softmax probs ğŸ’¡
            # apply nucleus filtering safely
            probs = top_p_filter(probs, top_p_local) # Apply top-p filtering â˜¢ï¸

            # sample or greedy depending on temperature (if temperature very close to 0 -> greedy)
            if temperature_local < 1e-4: # Check temp value ğŸŒ¡ï¸
                next_token = torch.argmax(probs, dim=-1, keepdim=True) # Greedy selection ğŸ¥‡
            else:
                # ensure sum>0 then sample
                if probs.sum() <= 0: # Check prob sum â•
                    probs = torch.softmax(next_token_logits, dim=-1) # Softmax probs ğŸ’¡
                next_token = torch.multinomial(probs, 1)  # (1,1) Sample token ğŸ²

            next_word = idx2word[next_token.item()] # Get next word ğŸ’¬
            if (next_word in [EOS_TOKEN, PAD_TOKEN] or 
                (len(generated_words) > 3 and next_word in generated_words[-3:])): # Check tokens ğŸš«
                break # Exit loop ğŸ›‘

            tgt = torch.cat([tgt, next_token], dim=1) # Concatenate target â•
            generated_words.append(next_word) # Add word to list âœ…
            last_tokens.append(next_token.item()) # Add token ID ğŸ”¢

            if next_token.item() == word2idx[EOS_TOKEN]: # Check EOS token ğŸ
                break # Exit loop ğŸ›‘

    return " ".join(generated_words) # Join words with space ğŸ—£ï¸

# ------------------ Training or Inference ------------------
if TRAIN_MODE: # Training mode on? âœ…
    # training needs Mongo
    from pymongo import MongoClient # Import mongo client ğŸ’¾
    from dotenv import load_dotenv # Load environment variables âš™ï¸

    load_dotenv() # Load env vars âš™ï¸
    MONGO_URI = os.getenv("MONGO_URI") # Get Mongo URI ğŸ”—
    client = MongoClient(MONGO_URI) # Create mongo client ğŸ’¾
    db = client["lynqbit_db"] # Get database ğŸ’¾
    collection = db["training_data"] # Get collection ğŸ’¾

    # Load dataset
    data = list(collection.find()) # Load data from DB ğŸ’¾
    questions = [d["question"].lower().split() for d in data] # Get questions â“
    answers = [d["answer"].lower().split() for d in data] # Get answers ğŸ’¬

    print(f"Loaded {len(questions)} training examples") # Print data info â„¹ï¸

    # Vocabulary
    all_words = set() # Create word set ğŸ“
    for q in questions: # Iterate questions ğŸ”„
        all_words.update(q) # Add words to set âœ…
    for a in answers: # Iterate answers ğŸ”„
        all_words.update(a) # Add words to set âœ…
    all_words.update([SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]) # Add special tokens âœ…

    word2idx = {w: i for i, w in enumerate(sorted(all_words))} # Word to index mapping ğŸ”¢
    idx2word = {i: w for w, i in word2idx.items()} # Index to word mapping ğŸ’¬
    vocab_size = len(word2idx) # Get vocab size ğŸ“

    print(f"Vocabulary size: {vocab_size}") # Print vocab size â„¹ï¸
    print(f"Device: {device} | d_model: {d_model}") # Print device info â„¹ï¸

    # Prepare tensors
    tensors_q = [text_to_tensor(q + [EOS_TOKEN], word2idx) for q in questions] # Create question tensors âœï¸
    tensors_a = [text_to_tensor([SOS_TOKEN] + a + [EOS_TOKEN], word2idx) for a in answers] # Create answer tensors âœï¸

    q_batch_all = pad_sequence(tensors_q, batch_first=True, padding_value=word2idx[PAD_TOKEN]) # Pad questions ğŸ“
    a_batch_all = pad_sequence(tensors_a, batch_first=True, padding_value=word2idx[PAD_TOKEN]) # Pad answers ğŸ“

    q_batch_all = q_batch_all[:, :max_len] # Truncate questions âœ‚ï¸
    a_batch_all = a_batch_all[:, :max_len] # Truncate answers âœ‚ï¸

    # Model, optimizer, loss
    model = ImprovedTransformer(vocab_size, d_model, num_heads, num_layers, max_len).to(device) # Initialize model ğŸ¤–
    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) # Adam optimizer âš™ï¸
    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[PAD_TOKEN]) # Cross entropy loss ğŸ“‰
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.8) # LR scheduler â±ï¸
    epochs = 150 # Training epochs â³
    min_loss = float('inf') # Init min loss â™¾ï¸
    patience_counter = 0 # Init patience counter â³
    early_stopping_patience = 12 # Early stopping patience â³

    print("Starting training with improved transformer...") # Print start message â„¹ï¸

    for epoch in range(epochs): # Training loop â³
        model.train() # Set to train mode ğŸš¦
        total_loss = 0.0 # Initialize loss ğŸ“‰
        num_batches = 0 # Initialize batches count ğŸ”¢
        # shuffle
        perm = torch.randperm(q_batch_all.size(0)) # Shuffle indices ğŸ²
        q_batch_shuffled = q_batch_all[perm] # Shuffle questions ğŸ“¦
        a_batch_shuffled = a_batch_all[perm] # Shuffle answers ğŸ“¦

        for i in range(0, len(q_batch_shuffled), batch_size): # Batch loop ğŸ“¦
            q_batch = q_batch_shuffled[i:i+batch_size].to(device) # Get batch ğŸ“¦
            a_batch = a_batch_shuffled[i:i+batch_size].to(device) # Get batch ğŸ“¦

            src_padding_mask, tgt_padding_mask, tgt_mask = create_mask(q_batch, a_batch[:, :-1], word2idx) # Create masks ğŸ­
            optimizer.zero_grad() # Zero gradients ğŸ“‰
            output = model(q_batch, a_batch[:, :-1],
                           src_key_padding_mask=src_padding_mask,
                           tgt_key_padding_mask=tgt_padding_mask,
                           tgt_mask=tgt_mask)  # output: (B, T, V) Get model output ğŸ“¤

            # shift targets: predict a_batch[:,1:]
            loss = criterion(output.reshape(-1, vocab_size), a_batch[:, 1:].reshape(-1)) # Calculate loss ğŸ“‰
            loss.backward() # Backpropagate loss â†©ï¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) # Clip gradients âœ‚ï¸
            optimizer.step() # Update weights â¬†ï¸

            total_loss += loss.item() # Accumulate loss ğŸ“‰
            num_batches += 1 # Increment batch count ğŸ”¢

        avg_loss = total_loss / (num_batches if num_batches > 0 else 1) # Calculate avg loss â—
        scheduler.step(avg_loss) # Step scheduler â±ï¸

        # checkpointing
        if avg_loss < min_loss: # Check loss value ğŸ“‰
            min_loss = avg_loss # Update min loss ğŸ“‰
            patience_counter = 0 # Reset patience counter â³
            torch.save({ # Save model state ğŸ’¾
                'model_state': model.state_dict(), # Save model state ğŸ’¾
                'word2idx': word2idx, # Save word2idx mapping ğŸ”¢
                'idx2word': idx2word, # Save idx2word mapping ğŸ’¬
                'd_model': d_model, # Save d_model param ğŸ“
                'num_heads': num_heads, # Save num_heads param ğŸ§ 
                'num_layers': num_layers, # Save num_layers param ğŸ§±
                'max_len': max_len # Save max_len param ğŸ“
            }, 'best_model.pth')
        else:
            patience_counter += 1 # Increment patience counter â³

        # monitoring
        if (epoch + 1) % 5 == 0: # Print every 5 epochs ğŸ—“ï¸
            print(f"[Epoch {epoch+1}/{epochs}] avg loss: {avg_loss:.4f}") # Print loss value ğŸ“‰

        if (epoch + 1) % 10 == 0:  # every 10 epochs show samples
            test_prompts = ["hello", "who created you"] # Test prompts ğŸ’¬
            print() # Print empty line âšª
            model.eval() # Set to eval mode ğŸš¦
            for prompt in test_prompts: # Iterate prompts ğŸ”„
                reply = generate_coherent_response(prompt, model, word2idx, idx2word, max_length=30, temperature_local=1.0) # Generate reply ğŸ—£ï¸
                print(f"  '{prompt}' -> '{reply}'") # Print prompt and reply ğŸ’¬
            print() # Print empty line âšª
            model.train() # Set to train mode ğŸš¦

        if patience_counter >= early_stopping_patience: # Check patience count â³
            print(f"Early stopping at epoch {epoch+1}") # Print stop message ğŸ›‘
            break # Exit loop ğŸ›‘

    # load best model after training (optional) to be ready for chat within this script
    if os.path.exists('best_model.pth'): # Check file exists ğŸ“
        ckpt = torch.load('best_model.pth', map_location=device) # Load checkpoint ğŸ’¾
        model.load_state_dict(ckpt['model_state']) # Load model state ğŸ¤–
        model.eval() # Set to eval mode ğŸš¦
        print("Best model loaded into memory (ready for inference).") # Print load message â„¹ï¸

else:
    # ------------------ Inference only ------------------
    if not os.path.exists('best_model.pth'): # Check file exists ğŸ“
        raise FileNotFoundError("best_model.pth not found. Train first or provide checkpoint.") # Raise error âŒ
    checkpoint = torch.load('best_model.pth', map_location=device) # Load checkpoint ğŸ’¾
    word2idx = checkpoint['word2idx'] # Load word2idx mapping ğŸ”¢
    idx2word = checkpoint['idx2word'] # Load idx2word mapping ğŸ’¬
    vocab_size = len(word2idx) # Get vocab size ğŸ“

    model = ImprovedTransformer(vocab_size, d_model, num_heads, num_layers, max_len).to(device) # Initialize model ğŸ¤–
    model.load_state_dict(checkpoint['model_state']) # Load model state ğŸ¤–
    model.eval() # Set to eval mode ğŸš¦
    print("Loaded model + vocab from best_model.pth") # Print load message â„¹ï¸

# ------------------ Chat loop (only runs if TRAIN_MODE is False OR after training you want interactive session) --
if not TRAIN_MODE: # Check train mode ğŸš¦
    print("\nğŸ˜¼ Lynqbit is online! Type your message (or !exit to quit)\n") # Print welcome message ğŸ˜¼
    while True: # Chat loop ğŸ’¬
        user_input = input("You: ").strip().lower() # Get user input ğŸ—£ï¸
        if user_input == "!exit": # Check exit command ğŸšª
            break # Exit loop ğŸ›‘
        response = generate_coherent_response(user_input, model, word2idx, idx2word, max_length=30) # Generate response ğŸ—£ï¸
        if response: # Check response ğŸ’¬
            emojis = ['ğŸ˜¼', 'ğŸ¾', 'âš¡', 'ğŸ˜¹'] # Emojis list ğŸ¤©
            if random.random() > 0.2: # Random condition ğŸ²
                response += " " + random.choice(emojis) # Add emoji âœ…
            print(f"Lynqbit: {response}") # Print response ğŸ’¬
        else:
            print("Lynqbit: *processor purrs* Try asking me something else? ğŸ˜¼") # Print error message ğŸ˜¼